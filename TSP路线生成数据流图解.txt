
╔═══════════════════════════════════════════════════════════════════════════════╗
║                    TSP 路线生成完整数据流程图解                                  ║
╚═══════════════════════════════════════════════════════════════════════════════╝

┌─────────────────────────────────────────────────────────────────────────────┐
│ 第1步：生成TSP问题实例                                                         │
└─────────────────────────────────────────────────────────────────────────────┘

    代码: td_init = env.reset(batch_size=[3]).to(device)
    
    输出数据结构：
    ┌───────────────────────────────────────┐
    │ TensorDict (batch_size=3)            │
    │  ├─ locs: Tensor[3, 50, 2]           │  ← 3个问题，每个50个城市，2D坐标
    │  ├─ action_mask: Tensor[3, 50]       │  ← 哪些城市可以访问
    │  └─ current_node: Tensor[3]          │  ← 当前位置（都是0号城市）
    └───────────────────────────────────────┘
    
    城市坐标示例（问题1）：
    ┌──────────────────────────┐
    │ City 0: [0.23, 0.67]    │
    │ City 1: [0.89, 0.12]    │
    │ City 2: [0.45, 0.91]    │
    │        ...               │
    │ City 49: [0.71, 0.38]   │
    └──────────────────────────┘


┌─────────────────────────────────────────────────────────────────────────────┐
│ 第2步：随机策略生成（Sampling解码）                                            │
└─────────────────────────────────────────────────────────────────────────────┘

    代码: policy(td_init.clone(), phase="test", decode_type="sampling", ...)
    
    ┌─────────────────┐
    │  输入: 城市坐标  │
    └────────┬────────┘
             ↓
    ┌──────────────────────────────────────────────┐
    │      Attention Model 策略网络                │
    │                                              │
    │  循环50次（每次选择一个城市）                 │
    │  ┌──────────────────────────────────────┐  │
    │  │ 步骤 i: 选择第i个要访问的城市          │  │
    │  │                                      │  │
    │  │  当前状态 → Encoder → Attention      │  │
    │  │                  ↓                   │  │
    │  │          输出概率分布：               │  │
    │  │          [0.05, 0.32, 0.15, 0.08...] │  │
    │  │                  ↓                   │  │
    │  │          📊 Sampling（采样）          │  │
    │  │          按概率随机选择               │  │
    │  │                  ↓                   │  │
    │  │          选中某个城市                 │  │
    │  └──────────────────────────────────────┘  │
    └──────────────────────────────────────────────┘
             ↓
    ┌──────────────────────────────────────────┐
    │  输出: 访问序列（actions）                │
    │  [0, 23, 8, 41, 15, 37, ...]            │
    │                                          │
    │  Reward（路径长度的负值）                 │
    │  -12.5834                                │
    └──────────────────────────────────────────┘


┌─────────────────────────────────────────────────────────────────────────────┐
│ 第3步：训练后策略生成（Greedy解码）                                            │
└─────────────────────────────────────────────────────────────────────────────┘

    代码: policy(td_init.clone(), phase="test", decode_type="greedy", ...)
    
    ┌─────────────────┐
    │  输入: 城市坐标  │  ← 相同的城市坐标
    └────────┬────────┘
             ↓
    ┌──────────────────────────────────────────────┐
    │      Attention Model 策略网络（训练后）       │
    │                                              │
    │  循环50次（每次选择一个城市）                 │
    │  ┌──────────────────────────────────────┐  │
    │  │ 步骤 i: 选择第i个要访问的城市          │  │
    │  │                                      │  │
    │  │  当前状态 → Encoder → Attention      │  │
    │  │                  ↓                   │  │
    │  │          输出概率分布：               │  │
    │  │          [0.02, 0.78, 0.05, 0.03...] │  │ ← 训练后分布更集中
    │  │                  ↓                   │  │
    │  │          🎯 Greedy（贪心）            │  │
    │  │          选择概率最大的               │  │
    │  │                  ↓                   │  │
    │  │          选中城市1（0.78最大）        │  │
    │  └──────────────────────────────────────┘  │
    └──────────────────────────────────────────────┘
             ↓
    ┌──────────────────────────────────────────┐
    │  输出: 优化后的访问序列                   │
    │  [0, 3, 7, 12, 19, 25, ...]             │  ← 更优的顺序
    │                                          │
    │  Reward（更短的路径）                     │
    │  -8.2341                                 │  ← 比随机策略好！
    └──────────────────────────────────────────┘


┌─────────────────────────────────────────────────────────────────────────────┐
│ 第4步：可视化对比                                                              │
└─────────────────────────────────────────────────────────────────────────────┘

    代码: env.render(td, actions, ax)
    
    ┌────────────────────────────────────────────────────────────────────┐
    │                    生成对比图片                                     │
    │                                                                    │
    │  左图：随机策略                   右图：训练后策略                    │
    │  ┌──────────────────────┐       ┌──────────────────────┐        │
    │  │ Random | Cost=12.58  │       │ Trained | Cost=8.23  │        │
    │  │                      │       │                      │        │
    │  │   ●───●              │       │    ●───●             │        │
    │  │   │╲ ╱│              │       │    │   │             │        │
    │  │   │ ╳ │  ← 路径交叉   │       │    ●───●  ← 路径顺畅  │        │
    │  │   │╱ ╲│              │       │    │   │             │        │
    │  │   ●───●              │       │    ●───●             │        │
    │  │                      │       │                      │        │
    │  └──────────────────────┘       └──────────────────────┘        │
    │                                                                    │
    │  保存为: comparison_6c61a541_1.png                                │
    └────────────────────────────────────────────────────────────────────┘


┌─────────────────────────────────────────────────────────────────────────────┐
│ 第5步：返回结果给前端                                                          │
└─────────────────────────────────────────────────────────────────────────────┘

    代码: queue.put(json.dumps({'type': 'complete', 'results': {...}}))
    
    ┌──────────────────────────────────────────────────────────┐
    │  JSON 数据结构                                            │
    │  {                                                       │
    │    "type": "complete",                                   │
    │    "message": "训练完成！",                               │
    │    "results": {                                          │
    │      "model": "attention",                               │
    │      "problem": "tsp",                                   │
    │      "strategy": "REINFORCE",                            │
    │      "total_epochs": 100,                                │
    │      "final_loss": 2.3456,                               │
    │      "final_reward": -8.2341,                            │
    │      "best_reward": -8.1234,                             │
    │      "plot_paths": [                                     │
    │        "/static/model_plots/comparison_6c61a541_1.png",  │
    │        "/static/model_plots/comparison_6c61a541_2.png",  │
    │        "/static/model_plots/comparison_6c61a541_3.png"   │
    │      ],                                                  │
    │      "checkpoint_path": "checkpoints/tsp-attention.ckpt" │
    │    }                                                     │
    │  }                                                       │
    └──────────────────────────────────────────────────────────┘
             ↓
    ┌──────────────────────────────────────────────────────────┐
    │  前端通过 SSE 接收数据                                     │
    │  JavaScript 动态更新页面                                  │
    │  显示3张对比图片                                           │
    └──────────────────────────────────────────────────────────┘


╔═══════════════════════════════════════════════════════════════════════════╗
║                        关键技术点解析                                      ║
╚═══════════════════════════════════════════════════════════════════════════╝

┌───────────────────────────────────────────────────────────────────────────┐
│ 1. Sampling vs Greedy 解码对比                                            │
└───────────────────────────────────────────────────────────────────────────┘

    策略网络输出的概率分布示例：
    
    城市 ID:     1      2      3      4      5    ...
               ──────────────────────────────────────
    Sampling:  0.15   0.32   0.08   0.25   0.20  ...  → 按概率随机选
                      ↑ 可能选这个
                                    ↑ 也可能选这个
    
    Greedy:    0.05   0.78   0.02   0.10   0.05  ...  → 选概率最大
                      ↑ 一定选这个（0.78最大）


┌───────────────────────────────────────────────────────────────────────────┐
│ 2. 注意力机制如何工作                                                      │
└───────────────────────────────────────────────────────────────────────────┘

    在每一步选择城市时：
    
    ┌──────────────────┐
    │  输入特征         │
    │  • 当前位置       │
    │  • 已访问城市     │
    │  • 未访问城市坐标  │
    └────────┬─────────┘
             ↓
    ┌────────────────────────────────────┐
    │  编码器（Encoder）                  │
    │  将所有城市编码为高维向量            │
    │  学习城市之间的全局关系              │
    └────────┬───────────────────────────┘
             ↓
    ┌────────────────────────────────────┐
    │  注意力层（Attention）               │
    │  计算当前位置对每个城市的"注意力"     │
    │  学会关注哪些城市应该优先访问         │
    └────────┬───────────────────────────┘
             ↓
    ┌────────────────────────────────────┐
    │  解码器（Decoder）                  │
    │  输出每个城市被选择的概率            │
    └────────┬───────────────────────────┘
             ↓
         概率分布


┌───────────────────────────────────────────────────────────────────────────┐
│ 3. 为什么训练后的路线更好？                                                 │
└───────────────────────────────────────────────────────────────────────────┘

    REINFORCE 学习过程：
    
    训练样本 1: 生成路线A → 成本12.5 → Reward: -12.5 → 降低这种策略
    训练样本 2: 生成路线B → 成本8.2  → Reward: -8.2  → 增强这种策略 ✓
    训练样本 3: 生成路线C → 成本15.3 → Reward: -15.3 → 降低这种策略
    ...
    训练10000次后 → 学会生成好的路线
    
    学到的模式：
    ✓ 优先访问附近的城市（近邻优先）
    ✓ 避免路径交叉
    ✓ 形成紧凑的路线
    ✓ 识别城市的聚类结构


┌───────────────────────────────────────────────────────────────────────────┐
│ 4. 数据维度变化                                                            │
└───────────────────────────────────────────────────────────────────────────┘

    输入:  [batch=3, cities=50, coords=2]  ← 3个问题，每个50城市
           ↓
    编码:  [batch=3, cities=50, embed=128] ← 嵌入到128维空间
           ↓
    注意力: [batch=3, cities=50, embed=128] ← 计算注意力权重
           ↓
    输出:  [batch=3, cities=50]            ← 每个城市的选择概率
           ↓
    选择:  [batch=3]                       ← 本步选择的城市ID
           ↓
    重复50次 →
           ↓
    路线:  [batch=3, cities=50]            ← 完整访问序列


╔═══════════════════════════════════════════════════════════════════════════╗
║                           性能对比示例                                     ║
╚═══════════════════════════════════════════════════════════════════════════╝

    问题1:
      随机策略: Cost = 12.5834  ◄─────┐
      训练后:   Cost = 8.2341         │ 改进 34.6%
                                      │
    问题2:                            │
      随机策略: Cost = 13.1247  ◄─────┤
      训练后:   Cost = 8.9156         │ 改进 32.1%
                                      │
    问题3:                            │
      随机策略: Cost = 11.8923  ◄─────┘
      训练后:   Cost = 8.4672         │ 改进 28.8%
      
    平均改进: 31.8% 🎉


╔═══════════════════════════════════════════════════════════════════════════╗
║                           文件存储位置                                     ║
╚═══════════════════════════════════════════════════════════════════════════╝

    项目根目录/
    ├── checkpoints/
    │   └── tsp-attention.ckpt           ← 训练好的模型权重
    │
    └── static/
        └── model_plots/
            ├── comparison_6c61a541_1.png  ← 问题1的对比图
            ├── comparison_6c61a541_2.png  ← 问题2的对比图
            └── comparison_6c61a541_3.png  ← 问题3的对比图

    session_id (6c61a541...) 确保不同训练的图片不会冲突

═══════════════════════════════════════════════════════════════════════════════

